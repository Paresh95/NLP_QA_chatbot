{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain # 0.0.333\n",
    "# !pip install sentence-transformers # 2.2.2\n",
    "# !pip install faiss-cpu # 1.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, split, embed and store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"data/Transcript Otter - A1.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "[Document(page_content=\"Unknown Speaker  1:19\\nHi Can you hear me\\n\\nUnknown Speaker  1:37\\nI can't hear you could be me. Hang on\\n\\nUnknown Speaker  1:56\\nknow let me\\n\\nSpeaker 1  2:07\\nknow Hello.\\n\\nSpeaker 2  2:13\\nI can hear you now that's fine I think it was my yes my speakers weren't working for some reason I think it's just been plugged in upstairs and I unplug it from my computer upstairs it kind of doesn't like it and messes around a little bit.\\n\\nSpeaker 1  2:25\\nThat's right, Mike, how are you? Yeah, good. Thank\\n\\nUnknown Speaker  2:28\\nyou. Oh by itself.\\n\\nSpeaker 1  2:30\\nYes. Not bad at all about all good stuff.\", metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 0, 'user_id': 1}), Document(page_content=\"Speaker 1  2:25\\nThat's right, Mike, how are you? Yeah, good. Thank\\n\\nUnknown Speaker  2:28\\nyou. Oh by itself.\\n\\nSpeaker 1  2:30\\nYes. Not bad at all about all good stuff.\\n\\nUnknown Speaker  2:33\\nHave you been up to my show?\\n\\nSpeaker 1  2:36\\nI went to the caravan at the weekend and I'm gonna go this weekend because the weather's so nice down here. I don't know what it's like.\\n\\nSpeaker 2  2:44\\nIt's yeah, it's raining currently, which isn't ideal. Yeah, so if you've got something's it's sunny down there at least dry?\", metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 425, 'user_id': 1})]\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "for doc in split_documents:\n",
    "    doc.metadata[\"user_id\"] = 1\n",
    "print(len(split_documents))\n",
    "print(split_documents[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/PareshSharma/.cache/torch/sentence_transformers/nreimers_MiniLM-L6-H384-uncased. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "), model_name='nreimers/MiniLM-L6-H384-uncased', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"nreimers/MiniLM-L6-H384-uncased\")\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(split_documents, embedding_model)\n",
    "db.save_local(\"data/vector_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_db = FAISS.load_local(\"data/vector_store\", embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find k neighbout documents given query and user id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content=\"Speaker 1  36:06\\nYes. So I might leave a little bit to my niece and nephews I don't know see how generous I'm feeling at 90\", metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 31591, 'user_id': 1}),\n",
       "  15.647223)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_db.similarity_search_with_score(\"how generous I'm feeling at\", k=1, filter=dict(user_id=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x12a1f7100>, search_kwargs={'filter': {'user_id': 1}, 'k': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_db.as_retriever(search_kwargs={'filter': {'user_id':1}, 'k': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "[Document(page_content=\"Unknown Speaker  1:19\\nHi Can you hear me\\n\\nUnknown Speaker  1:37\\nI can't hear you could be me. Hang on\\n\\nUnknown Speaker  1:56\\nknow let me\\n\\nSpeaker 1  2:07\\nknow Hello.\\n\\nSpeaker 2  2:13\\nI can hear you now that's fine I think it was my yes my speakers weren't working for some reason I think it's just been plugged in upstairs and I unplug it from my computer upstairs it kind of doesn't like it and messes around a little bit.\\n\\nSpeaker 1  2:25\\nThat's right, Mike, how are you? Yeah, good. Thank\\n\\nUnknown Speaker  2:28\\nyou. Oh by itself.\\n\\nSpeaker 1  2:30\\nYes. Not bad at all about all good stuff.\", metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 0, 'user_id': 2}), Document(page_content=\"Speaker 1  2:25\\nThat's right, Mike, how are you? Yeah, good. Thank\\n\\nUnknown Speaker  2:28\\nyou. Oh by itself.\\n\\nSpeaker 1  2:30\\nYes. Not bad at all about all good stuff.\\n\\nUnknown Speaker  2:33\\nHave you been up to my show?\\n\\nSpeaker 1  2:36\\nI went to the caravan at the weekend and I'm gonna go this weekend because the weather's so nice down here. I don't know what it's like.\\n\\nSpeaker 2  2:44\\nIt's yeah, it's raining currently, which isn't ideal. Yeah, so if you've got something's it's sunny down there at least dry?\", metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 425, 'user_id': 2})]\n"
     ]
    }
   ],
   "source": [
    "# see all documents in vectors store\n",
    "new_db.docstore._dict\n",
    "\n",
    "# get length of chunked documents in vector store \n",
    "len(new_db.docstore._dict)\n",
    "\n",
    "# add documents for a new user\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "for doc in split_documents:\n",
    "    doc.metadata[\"user_id\"] = 2\n",
    "print(len(split_documents))\n",
    "print(split_documents[0:2])\n",
    "\n",
    "\n",
    "# new_db.add_documents(split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"And you basically need to get another 22,000 pounds from some from somewhere. So effectively that monies then will then be subject to income tax. But you've still got your black, you know, Blackrock pension as well. And you've not taken you haven't taken your tax free cash from there yet have you know, so I think what was the is the real value of that plot about 100k Is it\", metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 13147, 'user_id': 2})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the document given the vector id\n",
    "index = faiss.read_index(\"data/vector_store/index.faiss\")\n",
    "vector_id = 42 # Get the vector ID to lookup \n",
    "vector = index.reconstruct(vector_id) # Reconstruct just the single vector for that ID\n",
    "vector_np = np.array([vector]) # Convert to numpy array\n",
    "# Search the index for nearest neighbor of the vector\n",
    "# This will return the original row for that vector ID\n",
    "_, I = index.search(vector_np, 1) \n",
    "row_id = I[0][0] # Get the row ID from the search results\n",
    "row_data = split_documents[row_id] # Now lookup the row data using the row ID\n",
    "row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151, 384)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all vectors back\n",
    "index = faiss.read_index(\"data/vector_store/index.faiss\")\n",
    "vectors = index.reconstruct_n(0, index.ntotal)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_transformers.long_context_reorder import LongContextReorder\n",
    "from langchain.retrievers.document_compressors.base import DocumentCompressorPipeline\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/flan-t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\", model_kwargs={\"max_new_tokens\": 50})\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = new_db.as_retriever(search_kwargs={'filter': {'user_id': 1}, 'k': 5})\n",
    "memory = ConversationBufferMemory(\n",
    "    llm=llm, memory_key=\"chat_history\", return_messages=True, output_key='answer'\n",
    ")\n",
    "\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say \"This wasn't discussed in the call.\", don't try to make up an answer. \n",
    "Use three sentences maximum and keep the answer as concise as possible. \n",
    "Chat History: {chat_history}\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['chat_history', 'context', 'question'])\n",
    "\n",
    "\n",
    "reordering = LongContextReorder()\n",
    "\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[\n",
    "        reordering\n",
    "    ]\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=pipeline_compressor, base_retriever=retriever)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=compression_retriever, memory=memory, return_source_documents=True, combine_docs_chain_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PareshSharma/miniforge3/envs/gpu-env/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What age might they leave something to their nephews and nieces?',\n",
       " 'chat_history': [HumanMessage(content='What age might they leave something to their nephews and nieces?'),\n",
       "  AIMessage(content='90.')],\n",
       " 'answer': '90.',\n",
       " 'source_documents': [Document(page_content=\"Speaker 1  36:06\\nYes. So I might leave a little bit to my niece and nephews I don't know see how generous I'm feeling at 90\", metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 31591, 'user_id': 1}),\n",
       "  Document(page_content=\"guys and see what kind of situation would be around that. And I think obviously for you know, for your kind of kind of stage at the moment, he probably wouldn't if you can do it without him or it's obviously the best case scenario.\", metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 18049, 'user_id': 1}),\n",
       "  Document(page_content=\"Speaker 1  28:52\\nI put, yeah, purchase it for 60. But the site fee was 2000 for this year, but I don't know if actual caravans lose their money or they increase I don't know probably lose their money.\", metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 25759, 'user_id': 1}),\n",
       "  Document(page_content='know, maybe sit down with Canada think about it and maybe just start planning out you know, the next year or so and get some things in the diary like holidays and trips to the caravan and all that type of stuff.', metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 42776, 'user_id': 1}),\n",
       "  Document(page_content=\"Speaker 1  33:36\\nI think you'd probably keep working a bit longer but we'll just leave it like this at the moment.\", metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 29180, 'user_id': 1})]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = \"What age might they leave something to their nephews and nieces?\"\n",
    "results = qa({\"question\": query})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text2text generation - google model - xl \n",
    "# table of other models tried - question-answering, text-generation\n",
    "\n",
    "# QA retrival\n",
    "# conversational retrival with history from memory, long context retrieval: https://github.com/langchain-ai/langchain/issues/10834\n",
    "# feed in prompt to conversational retriver\n",
    "# prompt template - message if error, answer, question, context, chat_history\n",
    "\n",
    "# save model and tokenizer and embeddings\n",
    "# retriever, \n",
    "# while loop for other code, \n",
    "# know how to reset memory\n",
    "# evaluation with ragas\n",
    "\n",
    "tokenizer.max_length\n",
    "\n",
    "\n",
    "\n",
    "# Optimisation\n",
    "# Chunk size \n",
    "# model\n",
    "# embeddings\n",
    "# tokenizer\n",
    "# chain type\n",
    "# search type\n",
    "# splitting function\n",
    "# vector db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# To get original document back look into retrievers in langchain\n",
    "# can use hugging face tokenizer for splitter? Increase chunk size, use tokens\n",
    "# what are models like misteral and llma used for? Can I get embeddings?\n",
    "# Will have to see if when we retrieve we can filter on the documents by user id, if not may need to use logic above to read in index (use langchain search feature to work this out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "# popular sentence transformer and high performing: sentence-transformers/all-mpnet-base-v2\n",
    "# High performing and trained on QA dataset: sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
    "# Smaller sentence embedding model 80MB: nreimers/MiniLM-L6-H384-uncased\n",
    "# Smaller sentence embedding model 290MB: sentence-transformers/all-distilroberta-v1\n",
    "# Popular QA model: deepset/roberta-base-squad2\n",
    "\n",
    "\n",
    "\n",
    "# vector store - chose FAISS as it is open source. For production use case weaviate or pinecone could also be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at previous langchain approach\n",
    "# look at example RAG systems online\n",
    "\n",
    "\n",
    "# load data into vector db with user id\n",
    "\n",
    "# Loader for data or textloader\n",
    "\n",
    "# splitting\n",
    "# embed data\n",
    "# add document to vector db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
