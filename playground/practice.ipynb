{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain # 0.0.333\n",
    "# !pip install sentence-transformers # 2.2.2\n",
    "# !pip install faiss-cpu # 1.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, split, embed and store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"data/Transcript Otter - A1.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672\n",
      "[Document(page_content='Unknown Speaker  1:19  \\nHi Can you hear me', metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 0, 'user_id': 1}), Document(page_content=\"Unknown Speaker  1:37  \\nI can't hear you could be me. Hang on\\n\\nUnknown Speaker  1:56  \\nknow let me\", metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 44, 'user_id': 1})]\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "for doc in split_documents:\n",
    "    doc.metadata[\"user_id\"] = 1\n",
    "print(len(split_documents))\n",
    "print(split_documents[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/PareshSharma/.cache/torch/sentence_transformers/nreimers_MiniLM-L6-H384-uncased. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "), model_name='nreimers/MiniLM-L6-H384-uncased', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"nreimers/MiniLM-L6-H384-uncased\")\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(split_documents, embedding_model)\n",
    "db.save_local(\"data/vector_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_db = FAISS.load_local(\"data/vector_store\", embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find k neighbout documents given query and user id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content=\"how generous I'm feeling at 90\", metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 31884, 'user_id': 1}),\n",
       "  2.940438)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_db.similarity_search_with_score(\"how generous I'm feeling at\", k=1, filter=dict(user_id=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x2bb21d720>, search_kwargs={'filter': {'user_id': 1}, 'k': 1})"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_db.as_retriever(search_kwargs={'filter': {'user_id':1}, 'k': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content=\"how generous I'm feeling at 90\", metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 31884, 'user_id': 1}),\n",
       "  2.940438)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see all documents in vectors store\n",
    "new_db.docstore._dict\n",
    "\n",
    "# get length of chunked documents in vector store \n",
    "len(new_db.docstore._dict)\n",
    "\n",
    "# add documents for a new user\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "for doc in split_documents:\n",
    "    doc.metadata[\"user_id\"] = 2\n",
    "print(len(split_documents))\n",
    "print(split_documents[0:2])\n",
    "\n",
    "\n",
    "new_db.add_documents(split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='it. Yeah, I will say it in my behalf a bit like my dogs fight just like the dogs. They just need to', metadata={'source': 'data/Transcript Otter - A1.txt', 'start_index': 2565, 'user_id': 1})"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the document given the vector id\n",
    "index = faiss.read_index(\"data/vector_store/index.faiss\")\n",
    "vector_id = 42 # Get the vector ID to lookup \n",
    "vector = index.reconstruct(vector_id) # Reconstruct just the single vector for that ID\n",
    "vector_np = np.array([vector]) # Convert to numpy array\n",
    "# Search the index for nearest neighbor of the vector\n",
    "# This will return the original row for that vector ID\n",
    "_, I = index.search(vector_np, 1) \n",
    "row_id = I[0][0] # Get the row ID from the search results\n",
    "row_data = split_documents[row_id] # Now lookup the row data using the row ID\n",
    "row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(672, 384)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all vectors back\n",
    "index = faiss.read_index(\"data/vector_store/index.faiss\")\n",
    "vectors = index.reconstruct_n(0, index.ntotal)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x2bb21d720>, search_kwargs={'filter': {'user_id': 1}})"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# To get original document back look into retrievers in langchain\n",
    "# can use hugging face tokenizer for splitter? Increase chunk size, use tokens\n",
    "# what are models like misteral and llma used for? Can I get embeddings?\n",
    "# Will have to see if when we retrieve we can filter on the documents by user id, if not may need to use logic above to read in index (use langchain search feature to work this out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "# popular sentence transformer and high performing: sentence-transformers/all-mpnet-base-v2\n",
    "# High performing and trained on QA dataset: sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
    "# Smaller sentence embedding model 80MB: nreimers/MiniLM-L6-H384-uncased\n",
    "# Smaller sentence embedding model 290MB: sentence-transformers/all-distilroberta-v1\n",
    "# Popular QA model: deepset/roberta-base-squad2\n",
    "\n",
    "\n",
    "\n",
    "# vector store - chose FAISS as it is open source. For production use case weaviate or pinecone could also be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at previous langchain approach\n",
    "# look at example RAG systems online\n",
    "\n",
    "\n",
    "# load data into vector db with user id\n",
    "\n",
    "# Loader for data or textloader\n",
    "\n",
    "# splitting\n",
    "# embed data\n",
    "# add document to vector db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
